?worldcup
summary(model2)
summary(model2)$coef[1]
summary(model2)$coef[2]
summary(model2)$coef[2] * 100
[summary(model2)$coef[2] * 100]^2
(summary(model2)$coef[2] * 100)^2
summary(model2)$coef[2] + 1
(summary(model2)$coef[2] + 1)^2
-1
(summary(model2)$coef[2] + 1)^2 -1
((summary(model2)$coef[2] + 1)^2 -1) * 100
summary(model2)
view(melanoma)
view(Melanoma)
?Melanoma
Melanoma$year
max(Melanoma$year)
model$coef[2]
model2$coef[2]
model2 <- glm(I(status!=1)~year+ulcer,family=binomial,data=Melanoma)
summary(model2)
model2$coefficients
model2$coefficients[0]
model2$coefficients[1]
model2$coefficients[2]
model2$coefficients[2]+1
# Q3
model2$coefficients[2] * 100
# Q4
(model2$coefficients[2] + 1)^2
(a - 1 ) * 100
# Q4
a = (model2$coefficients[2] + 1)^2
(a - 1 ) * 100
summary(model2)
# Q7
predict(model2,newdata=data.frame(year=1970,ulcer=1),type='response')-
predict(model2,newdata=data.frame(year=1970,ulcer=0),type='response')
predict(model2,newdata=data.frame(year=1970,ulcer=1),type='response')-
predict(model2,newdata=data.frame(year=1970,ulcer=0),type='response')
predict(model2,newdata=data.frame(year=1970,ulcer=1),type='response')-
predict(model2,newdata=data.frame(year=1970,ulcer=0),type='response')
model2$coefficients[2]
# Q7
predict(model2,newdata=data.frame(year=1970,ulcer=1),type='response')-
predict(model2,newdata=data.frame(year=1970,ulcer=0),type='response')
# Q8
predict(model2,newdata=data.frame(year=1975,ulcer=1),type='response')-
predict(model2,newdata=data.frame(year=1975,ulcer=0),type='response')
?worldcup
# From Q9 to Q13
wc2 <- worldcup
wc2$midfielder <- wc2$Position=='Midfielder'
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
wc2
# From Q9 to Q13
wc2 <- worldcup
wc2$midfielder <- wc2$Position=='Midfielder'
wc2[,3]
wc2
wc2$midfielder <- wc2$Position=='Midfielder'
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
wc.glm <- glm(
midfielder~Shots+Passes+Tackles,
family=binomial,
wc2)
summary(wc.glm)
summary(wc.glm)
mean(wc2$midfielder==round(predict(wc.glm,type='response')))
wc.glm,type='response'
wc.glm,type='response'
predict(wc.glm,type='response')
table(wc2$Position)
1-table(wc2$Position)[4]/dim(wc2)[1]
?table
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
# From Q9 to Q13
wc2 <- worldcup
wc2$midfielder <- wc2$Position=='Midfielder'
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
wc.glm <- glm(
midfielder~Shots+Passes+Tackles,
family=binomial,
wc2)
summary(wc.glm)
mean(wc2$midfielder==round(predict(wc.glm,type='response')))
# Q11
table(wc2$Position)
1-table(wc2$Position)[4]/dim(wc2)[1]
# Q12
wc.glm2 <- glm(midfielder~Shots+Passes+Tackles+Saves,
family=binomial,
wc2)
wc.glm3 <- glm(midfielder~Shots+Passes+Tackles,
family=binomial,
weights=Time,
wc2)
# Q12
wc.glm2 <- glm(midfielder~Shots+Passes+Tackles+Saves,
family=binomial,
wc2)
wc.glm3 <- glm(midfielder~Shots+Passes+Tackles,
family=binomial,
weights=Time,
wc2)
# From Q9 to Q13
wc2 <- worldcup
wc2$midfielder <- wc2$Position=='Midfielder'
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
wc.glm <- glm(
midfielder~Shots+Passes+Tackles,
family=binomial,
wc2)
summary(wc.glm)
mean(wc2$midfielder==round(predict(wc.glm,type='response')))
# Q11
table(wc2$Position)
1-table(wc2$Position)[4]/dim(wc2)[1]
acc(wc.glm)
# From Q9 to Q13
wc2 <- worldcup
wc2$midfielder <- wc2$Position=='Midfielder'
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
wc.glm <- glm(
midfielder~Shots+Passes+Tackles,
family=binomial,
wc2)
summary(wc.glm)
mean(wc2$midfielder==round(predict(wc.glm,type='response')))
# Q11
table(wc2$Position)
1-table(wc2$Position)[4]/dim(wc2)[1]
# Q12
wc.glm2 <- glm(midfielder~Shots+Passes+Tackles+Saves,
family=binomial,
wc2)
wc.glm3 <- glm(midfielder~Shots+Passes+Tackles,
family=binomial,
weights=Time,
wc2)
acc <- function(model){
acc1 <- mean(wc2$midfielder==round(predict(model,type='response')))
confmat <- table(wc2$midfielder,round(predict(model,type='response')))
acc2 <- mean(c(confmat[1,1]/sum(confmat[1,]),confmat[2,2]/sum(confmat[2,])))
return(c(accuracy=acc1,bal.acc=acc2))}
acc(wc.glm)
acc(wc.glm2)
acc(wc.glm3)
wc.nopredictor.glm <- glm(
midfielder~,
# From Q9 to Q13
wc2 <- worldcup
wc2$midfielder <- wc2$Position=='Midfielder'
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
wc.glm <- glm(
midfielder~Shots+Passes+Tackles,
family=binomial,
wc2)
summary(wc.glm)
round(predict(wc.glm,type='response')))
round(predict(wc.glm,type='response'))
?round
mean(wc2$midfielder==round(predict(wc.glm,type='response')))
# Q11
table(wc2$Position)
1-table(wc2$Position)[4]/dim(wc2)[1]
/dim(wc2)[1]
dim(wc2)[1]
dim(wc2)
wc2 <- worldcup
wc2$midfielder <- wc2$Position=='Midfielder'
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
wc.glm <- glm(
midfielder~Shots+Passes+Tackles,
family=binomial,
wc2)
summary(wc.glm)
mean(wc2$midfielder==round(predict(wc.glm,type='response')))
# Q11
table(wc2$Position)
1-table(wc2$Position)[4]/dim(wc2)[1]
# Q12
wc.glm2 <- glm(midfielder~Shots+Passes+Tackles+Saves,
family=binomial,
wc2)
wc.glm3 <- glm(midfielder~Shots+Passes+Tackles,
family=binomial,
weights=Time,
wc2)
acc <- function(model){
acc1 <- mean(wc2$midfielder==round(predict(model,type='response')))
confmat <- table(wc2$midfielder,round(predict(model,type='response')))
acc2 <- mean(c(confmat[1,1]/sum(confmat[1,]),
confmat[2,2]/sum(confmat[2,])))
return(c(accuracy=acc1,bal.acc=acc2))}
acc(wc.glm)
acc(wc.glm2)
acc(wc.glm3)
table(wc2$Position)
1-table(wc2$Position)[4]/dim(wc2)[1]
mean(wc2$midfielder==round(predict(wc.glm,type='response')))
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
wc.glm <- glm(
midfielder~Shots+Passes+Tackles,
family=binomial,
wc2)
model1 <- glm(I(status!=1)~sex+age+year+thickness+ulcer,family=binomial,data=Melanoma)
summary(model1)
model2 <- glm(I(status!=1)~year+ulcer,family=binomial,data=Melanoma)
summary(model2)
# Q4
# 1.64777
a = (model2$coefficients[2] + 1)^2
(a - 1 ) * 100
# Q7
predict(model2,newdata=data.frame(year=1970,ulcer=1),type='response')-
predict(model2,newdata=data.frame(year=1970,ulcer=0),type='response')
# Q8
predict(model2,newdata=data.frame(year=1975,ulcer=1),type='response')-
predict(model2,newdata=data.frame(year=1975,ulcer=0),type='response')
acc(wc.glm)
# From Q9 to Q13
wc2 <- worldcup
wc2$midfielder <- wc2$Position=='Midfielder'
wc2[,4:7] <- wc2[,4:7]/wc2[,3]
wc.glm <- glm(
midfielder~Shots+Passes+Tackles,
family=binomial,
wc2)
summary(wc.glm)
mean(wc2$midfielder==round(predict(wc.glm,type='response')))
# Q11
table(wc2$Position)
1-table(wc2$Position)[4]/dim(wc2)[1]
# Q12
wc.glm2 <- glm(midfielder~Shots+Passes+Tackles+Saves,
family=binomial,
wc2)
wc.glm3 <- glm(midfielder~Shots+Passes+Tackles,
family=binomial,
weights=Time,
wc2)
acc <- function(model){
acc1 <- mean(wc2$midfielder==round(predict(model,type='response')))
confmat <- table(wc2$midfielder,round(predict(model,type='response')))
acc2 <- mean(c(confmat[1,1]/sum(confmat[1,]),
confmat[2,2]/sum(confmat[2,])))
return(c(accuracy=acc1,bal.acc=acc2))}
acc(wc.glm)
acc(wc.glm2)
acc(wc.glm3)
model1 <- glm(I(status!=1)~sex+age+year+thickness+ulcer,family=binomial,data=Melanoma)
summary(model1)
model2 <- glm(I(status!=1)~year+ulcer,family=binomial,data=Melanoma)
summary(model2)
model1 <- glm(I(status!=1)~sex+age+year+thickness+ulcer,family=binomial,data=Melanoma)
summary(model1)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(GGally) ##plot correlation
library(Amelia)
library(Hmisc)
library(lmtest)
library(MASS)
library(faraway)
library(reshape2)
# setwd('C:\\Prinu\\Personal\\Studies\\Masters\\UChicago\\After Admission\\Courses\\Statistical Analysis\\Assignments\\Final Assignment - Part1\\data')
setwd("C:/Users/QZHAN101/Downloads/play/UCHICAGO/MSCA_31007_ON01_Autumn_2022_Statistical_Analysis/Statistical_Analysis_final_project")
#*******************************Step 1: Import and prepare the data for analysis*******************************#
#1.1 Bring the data into R
#Using R, bring all five datasets into your workspace.
# Notice that all five datasets have 21 columns,
#with similar (but not identical) column names.  Please use the following
# vector of column names to standardize the data.
brooklyn_2016 <- read.csv('2016_brooklyn.csv')
brooklyn_2017 <- read.csv('2017_brooklyn.csv')
brooklyn_2018 <- read.csv('2018_brooklyn.csv')
brooklyn_2019 <- read.csv('2019_brooklyn.csv')
brooklyn_2020 <- read.csv('2020_brooklyn.csv')
#removing first 'n' rows from csv files as those are description entries
brooklyn_2016 <- tail(brooklyn_2016, -4)
brooklyn_2017 <- tail(brooklyn_2017, -4)
brooklyn_2018 <- tail(brooklyn_2018, -4)
brooklyn_2019 <- tail(brooklyn_2019, -4)
brooklyn_2020 <- tail(brooklyn_2020, -7)
#define column names for each dataframes
colnames <- c(
"borough","neighborhood","bldclasscat","taxclasscurr",
"block","lot","easement","bldclasscurr","address",
"aptnum","zip","resunits","comunits","totunits",
"landsqft","grosssqft","yrbuilt","taxclasssale",
"bldclasssale","price","date"
)
colnames(brooklyn_2016) <- colnames
colnames(brooklyn_2017) <- colnames
colnames(brooklyn_2018) <- colnames
colnames(brooklyn_2019) <- colnames
colnames(brooklyn_2020) <- colnames
#define rows names for each dataframes
rownames(brooklyn_2016) <- 1:nrow(brooklyn_2016)
rownames(brooklyn_2017) <- 1:nrow(brooklyn_2017)
rownames(brooklyn_2018) <- 1:nrow(brooklyn_2018)
rownames(brooklyn_2019) <- 1:nrow(brooklyn_2019)
rownames(brooklyn_2020) <- 1:nrow(brooklyn_2020)
#1.2 Join the data and make it usable for analysis
#There are some data cleaning steps and transformations that would be
# necessary or helpful to almost any analysis.
#Consider your data carefully, column by column.  Re-format, change data
# types, pay attention to white space and
#special characters.  Datasets kept over multiple years are not always
# created in exactly the same way, so take
#care that your data is standardized across years.  When you are done,
# create a new datasets which joins all
#five yearly datasets.  This step will likely take a large amount of time.
# Do not assume you can complete it quickly.
#The resulting dataset should have roughly 119,000 rows.
#remove empty rows
brooklyn_2016 <- brooklyn_2016[!apply(brooklyn_2016 == "", 1, all),]
brooklyn_2017 <- brooklyn_2017[!apply(brooklyn_2017 == "", 1, all),]
brooklyn_2018 <- brooklyn_2018[!apply(brooklyn_2018 == "", 1, all),]
brooklyn_2019 <- brooklyn_2019[!apply(brooklyn_2019 == "", 1, all),]
brooklyn_2020 <- brooklyn_2020[!apply(brooklyn_2020 == "", 1, all),]
#since we removed blank rows from brooklyn 2019 & 2020, we reduced
# from 119351 rows to 117151
total_observations <- nrow(brooklyn_2016) +
nrow(brooklyn_2017) +
nrow(brooklyn_2018) +
nrow(brooklyn_2019) +
nrow(brooklyn_2020)
#define functions.
#This one is for trim white space.
func.df.trim <- function(df, colnames) {
for (colname in colnames) {
df[[colname]] <- trimws(df[[colname]])
}
return(df)
}
func.df.replace <- function(df, colnames, oldvalue, newvalue) {
for (colname in colnames) {
df[[colname]][df[[colname]] == oldvalue] <- newvalue
}
return(df)
}
# convert element to integer.
func.df.ToInt <- function(df, colnames) {
for (colname in colnames) {
df[[colname]] <- as.integer(df[[colname]])
}
return(df)
}
# remove the comma in elements, e.g., "1,003" will be "1003"
func.df.ToNum <- function(df, colnames) {
for (colname in colnames) {
df[[colname]] <- str_replace_all(df[[colname]], "[^0-9.]", "")
df[[colname]] <- suppressWarnings(
as.numeric(gsub(",", "", format(df[[colname]], scientific = F)))
)
}
return(df)
}
# convert the date to a specific format.
func.df.ToDate <- function(df, colnames, format) {
for (colname in colnames) {
date_formatted <- as.POSIXct(df[[colname]], format=format)
df[[colname]] <- as.Date(date_formatted, format=format)
}
return(df)
}
#trim leading and trailing white spaces for columns
brooklyn_2016 <- func.df.trim(brooklyn_2016, colnames)
brooklyn_2017 <- func.df.trim(brooklyn_2017, colnames)
brooklyn_2018 <- func.df.trim(brooklyn_2018, colnames)
brooklyn_2019 <- func.df.trim(brooklyn_2019, colnames)
brooklyn_2020 <- func.df.trim(brooklyn_2020, colnames)
#replace column value '-' with empty
columns_with_hyphen = c('borough','resunits','comunits','totunits','landsqft','grosssqft','taxclasssale')
brooklyn_2016 <- func.df.replace(brooklyn_2016, columns_with_hyphen, '-', '')
brooklyn_2017 <- func.df.replace(brooklyn_2017, columns_with_hyphen, '-', '')
brooklyn_2018 <- func.df.replace(brooklyn_2018, columns_with_hyphen, '-', '')
brooklyn_2019 <- func.df.replace(brooklyn_2019, columns_with_hyphen, '-', '')
brooklyn_2020 <- func.df.replace(brooklyn_2020, columns_with_hyphen, '-', '')
#change data types for following columns
brooklyn_2016 <- func.df.ToInt(brooklyn_2016,list('borough','taxclasscurr','block','lot','resunits','comunits','totunits','yrbuilt','taxclasssale','zip'))
brooklyn_2016 <- func.df.ToNum(brooklyn_2016,list('landsqft','grosssqft','price'))
brooklyn_2016 <- func.df.ToDate(brooklyn_2016,list('date'),format="%m/%d/%Y")
brooklyn_2016 <- brooklyn_2016 %>% filter(!is.na(price))
brooklyn_2017 <- func.df.ToInt(brooklyn_2017,list('borough','taxclasscurr','block','lot','resunits','comunits','totunits','yrbuilt','taxclasssale','zip'))
brooklyn_2017 <- func.df.ToNum(brooklyn_2017,list('landsqft','grosssqft','price'))
brooklyn_2017 <- func.df.ToDate(brooklyn_2017,list('date'),format="%m/%d/%y")
brooklyn_2017 <- brooklyn_2017 %>% filter(!is.na(price))
brooklyn_2018 <- func.df.ToInt(brooklyn_2018,list('borough','taxclasscurr','block','lot','resunits','comunits','totunits','yrbuilt','taxclasssale','zip'))
brooklyn_2018 <- func.df.ToNum(brooklyn_2018,list('landsqft','grosssqft','price'))
brooklyn_2018 <- func.df.ToDate(brooklyn_2018,list('date'),format="%m/%d/%y")
brooklyn_2018 <- brooklyn_2018 %>% filter(!is.na(price))
brooklyn_2019 <- func.df.ToInt(brooklyn_2019,list('borough','taxclasscurr','block','lot','resunits','comunits','totunits','yrbuilt','taxclasssale','zip'))
brooklyn_2019 <- func.df.ToNum(brooklyn_2019,list('landsqft','grosssqft','price'))
brooklyn_2019 <- func.df.ToDate(brooklyn_2019,list('date'),format="%m/%d/%y")
brooklyn_2019 <- brooklyn_2019 %>% filter(!is.na(price))
brooklyn_2020 <- func.df.ToInt(brooklyn_2020,list('borough','taxclasscurr','block','lot','resunits','comunits','totunits','yrbuilt','taxclasssale','zip'))
brooklyn_2020 <- func.df.ToNum(brooklyn_2020,list('landsqft','grosssqft','price'))
brooklyn_2020 <- func.df.ToDate(brooklyn_2020,list('date'),format="%m/%d/%y")
brooklyn_2020 <- brooklyn_2020 %>% filter(!is.na(price))
#After doing data cleaning for brooklyn 2019 & 2020, we reduced from 117151 rows to 107270
total_observations_after_cleanup <- nrow(brooklyn_2016) +
nrow(brooklyn_2017) +
nrow(brooklyn_2018) +
nrow(brooklyn_2019) +
nrow(brooklyn_2020)
#merge the dataframes
brooklyn_2016_2020_list <- list(
brooklyn_2016,
brooklyn_2017,
brooklyn_2018,
brooklyn_2019,
brooklyn_2020
)
brooklyn_2016_2020 <- brooklyn_2016_2020_list %>% reduce(full_join)
remove(brooklyn_2016_2020_list)
#1.3 Filter the data and make transformations specific to this analysis
#For the purposes of this analysis, we will only consider purchases of single-family residences and single-unit apartments
#or condos.  Restrict the data to purchases where the building class at the time of sale starts with ‘A’ or ‘R’ and where
#the number of total units and the number of residential units are both 1.  Additionally restrict the data to observations
#where gross square footage is more than 0 and sale price is non-missing.  The resulting dataset should have roughly 19,000 rows.
#filter observations considering purchases of single-family residences and single-unit apartments or condos
#Restrict the data to purchases where the building class at the time of sale starts with ‘A’ or ‘R’
brooklyn_2016_2020_final <- brooklyn_2016_2020 %>%
filter(str_detect(bldclasssale, "^A") | str_detect(bldclasssale, "^R"))
#the number of total units and the number of residential units are both 1
brooklyn_2016_2020_final <- brooklyn_2016_2020_final %>%
filter(resunits == 1 & totunits == 1)
#additionally restrict the data to observation where gross square footage is more than 0
brooklyn_2016_2020_final <- brooklyn_2016_2020_final %>%
filter(grosssqft > 0 & !is.na(grosssqft))
#additionally restrict the data to observation where sale price is non-missing
#brooklyn_2016_2020_final[["price"]][is.na(brooklyn_2016_2020_final[["price"]])] <- 0
brooklyn_2016_2020_final <- brooklyn_2016_2020_final %>%
filter(!is.na(price))
#additionally restrict the data to observation where Year Built is more than 0
brooklyn_2016_2020_final <- brooklyn_2016_2020_final %>%
filter(yrbuilt > 0)
#additionally restrict the data to observation where price is less than 10 million. I would consider those as outliers
brooklyn_2016_2020_final <- brooklyn_2016_2020_final %>%
filter(price < 15000000)
#additionally restrict the data to observation where grosssqft is less than 20k. I would consider those as outliers
brooklyn_2016_2020_final <- brooklyn_2016_2020_final %>%
filter(grosssqft < 20000)
#2.1.1.1 - Statistics summary
dim(brooklyn_2016_2020_final)
summary(brooklyn_2016_2020_final)
#2.1.1.2 - Check for any NA’s in the dataframe
missmap(brooklyn_2016_2020_final,col=c('yellow','black'),y.at=1,y.labels='',legend=TRUE)
colSums(is.na(brooklyn_2016_2020_final))
#2.1.1.3 - Data Cleansing — Handle missing data
brooklyn_2016_2020_final[["taxclasscurr"]][str_detect(brooklyn_2016_2020_final[["bldclasssale"]], "^A") & is.na(brooklyn_2016_2020_final[["taxclasscurr"]])] <- 1
brooklyn_2016_2020_final[["taxclasscurr"]][str_detect(brooklyn_2016_2020_final[["bldclasssale"]], "^R") & is.na(brooklyn_2016_2020_final[["taxclasscurr"]])] <- 2
brooklyn_2016_2020_final[["comunits"]][is.na(brooklyn_2016_2020_final[["comunits"]])] <- 0
#2.1.1.4 - Correlations
#A positive correlation indicates the extent to which those variables increase or decrease in parallel;
#a negative correlation indicates the extent to which one variable increases as the other decreases.
ggcorr(brooklyn_2016_2020_final, label = T, hjust = 1, layout.exp = 3)
#2.1.1.2 - Check for any NA’s in the dataframe
missmap(brooklyn_2016_2020_final,col=c('yellow','black'),y.at=1,y.labels='',legend=TRUE)
colSums(is.na(brooklyn_2016_2020_final))
#2.1.1.5 - visualizing the distribution of the target variable 'price'
brooklyn_2016_2020_final %>%
ggplot(aes(price)) +
stat_density() +
theme_bw()
#2.1.1.6 - effect of the predictor variables on target variable 'price'
brooklyn_2016_2020_final %>%
dplyr::select(c(price,zip,neighborhood,block,grosssqft,landsqft,yrbuilt,bldclasssale,taxclasssale)) %>%
melt(id.vars = "price") %>%
ggplot(aes(x = value, y = price, colour = variable)) +
geom_point(alpha = 0.7) +
stat_smooth(aes(colour = "black")) +
facet_wrap(~variable, scales = "free", ncol = 2) +
labs(x = "Variable Value", y = "Price ($1000s)") +
theme_minimal()
#2.1.1.4 - Correlations
#A positive correlation indicates the extent to which those variables increase or decrease in parallel;
#a negative correlation indicates the extent to which one variable increases as the other decreases.
ggcorr(brooklyn_2016_2020_final, label = T, hjust = 1, layout.exp = 3)
#2.1.1.6 - effect of the predictor variables on target variable 'price'
brooklyn_2016_2020_final %>%
dplyr::select(c(price,zip,neighborhood,block,grosssqft,landsqft,yrbuilt,bldclasssale,taxclasssale)) %>%
melt(id.vars = "price") %>%
ggplot(aes(x = value, y = price, colour = variable)) +
geom_point(alpha = 0.7) +
stat_smooth(aes(colour = "black")) +
facet_wrap(~variable, scales = "free", ncol = 2) +
labs(x = "Variable Value", y = "Price ($1000s)") +
theme_minimal()
#2.1.3.1 - Additionally restrict the data to observation where price is greater than 0
brooklyn_2016_2020_final <- brooklyn_2016_2020_final %>% filter(price > 0 & !is.na(price))
#2.1.3.2 - Additionally restrict the data to observation where zip is greater than 0
brooklyn_2016_2020_final <- brooklyn_2016_2020_final %>% filter(zip > 0)
View(brooklyn_2016_2020_final)
#2.1.3.3 - visualizing the distribution of the target variable 'price'
brooklyn_2016_2020_final %>%
ggplot(aes(price)) +
stat_density() +
theme_bw()
View(brooklyn_2016_2020_final)
nrow(brooklyn_2016_2020_final)
nrow(brooklyn_2016_2020_final[brooklyn_2016_2020_final$price <10000])
nrow(brooklyn_2016_2020_final$price <10000)
(brooklyn_2016_2020_final$price <10000)
(brooklyn_2016_2020_final$price <10000)
sum((brooklyn_2016_2020_final$price <10000), na.rm = TRUE)
sum((brooklyn_2016_2020_final$price <1000), na.rm = TRUE)
